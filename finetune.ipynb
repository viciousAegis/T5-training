{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subtldev/T5-training/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from termcolor import colored\n",
    "import textwrap\n",
    "import random\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be85543aeaaa14008c9063',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'question': 'When did Beyonce start becoming popular?',\n",
       " 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_answer_start(example):\n",
    "    if(len(example['answers'])==0):\n",
    "        return \"Unanswerable\"\n",
    "    answer_start = example['answers']['answer_start'][0]\n",
    "    context = example['context']\n",
    "    return colored(context[:answer_start], 'white') + colored(context[answer_start:answer_start+2], 'red') + colored(context[answer_start+5:], 'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What effect did the 1936 Soviet Constitution have on the size of the Russia?\n",
      "Answer: \n",
      "\u001b[97mMany regions in Russia were affected by the Soviet famine of 1932–1933: Volga; Central Black\n",
      "Soil Region; North Caucasus; the Urals; the Crimea; part of Western Siberia; and the Kazak ASSR.\n",
      "With the adoption of the 1936 Soviet Constitution on December 5, 1936, the size of the RSFSR was\n",
      "significantly \u001b[0m\u001b[31mre\u001b[0m\u001b[97med. The Kazakh ASSR and Kirghiz ASSR were transformed into the\n",
      "Kazakh and Kirghiz Soviet Socialist Republics. The Karakalpak Autonomous Socialist Soviet Republic\n",
      "was transferred to the Uzbek SSR.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sample = dataset['train'][random.randint(0, len(dataset['train']))]\n",
    "print(\"Question: \", sample['question'])\n",
    "print(\"Answer: \")\n",
    "for wrap in textwrap.wrap(color_answer_start(sample), 100):\n",
    "    print(wrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "\n",
    "# have at most 1 answer for each question\n",
    "\n",
    "def preprocess_function(example):\n",
    "    if(len(example['answers']['answer_start'])==0):\n",
    "        example['answers']['answer_start'] = -1\n",
    "        example['answers']['text'] = 'Unanswerable'\n",
    "    else:\n",
    "        example['answers']['answer_start'] = example['answers']['answer_start'][0]\n",
    "        example['answers']['text'] = example['answers']['text'][0]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'google/flan-t5-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoding = tokenizer(sample['question'], sample['context'], truncation=True, padding='max_length', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "[363, 1504, 410, 8, 27598, 12873, 11378, 43, 30, 8, 812, 13, 8, 4623, 58, 1, 1404, 6266, 16, 4623, 130, 4161, 57, 8, 12873, 3, 89, 8721, 13, 957, 2668, 104, 2294, 4201, 10, 4969, 122, 9, 117, 2808, 1589, 264, 173, 6163, 117, 1117, 16371, 6769, 302, 117, 8, 4575, 5405, 117, 8, 16923, 9, 117, 294, 13, 3782, 925, 115, 4476, 117, 11, 8, 26094, 1639, 6157, 6857, 5, 438, 8, 9284, 13, 8, 27598, 12873, 11378, 30, 1882, 7836, 27598, 6, 8, 812, 13, 8, 391, 7016, 6857, 47, 4019, 3915, 5, 37, 26094, 18965, 6157, 6857, 11, 10976, 5649, 172, 6157, 6857, 130, 13421, 139, 8, 26094, 18965, 11, 10976, 5649, 172, 12873, 2730, 343, 5750, 7, 5, 37, 17422, 4766, 16864, 2040, 3114, 1162, 2730, 343, 12873, 5750, 47, 10250, 12, 8, 412, 172, 346, 157, 180, 6857, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(sample_encoding.keys())\n",
    "print(sample_encoding['input_ids'])\n",
    "print(sample_encoding['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What effect did the 1936 Soviet Constitution have on the size of the Russia ?  Many regions in Russia were affected by the Soviet  f amine of 19 32 – 19 33 : Vol g a ; Central Black So il Region ; North Cau cas us ; the Ur als ; the Crime a ; part of Western Si b eria ; and the Kaz ak AS SR . With the adoption of the 1936 Soviet Constitution on December 5, 1936 , the size of the R SF SR was significantly reduced . The Kaz akh AS SR and Kir ghi z AS SR were transformed into the Kaz akh and Kir ghi z Soviet Social ist Republic s . The Kara kal pak Auto nom ous Social ist Soviet Republic was transferred to the U z be k S SR .                                                                                                                                                                                                                                                                                                                                                                               '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [\n",
    "    tokenizer.decode(input_id, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    for input_id in sample_encoding['input_ids']\n",
    "]\n",
    "\" \".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(\n",
    "    sample['question'],\n",
    "    sample['context'],\n",
    "    truncation='only_second',\n",
    "    padding='max_length',\n",
    "    max_length=512,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>',\n",
       " 'additional_special_tokens': ['<extra_id_0>',\n",
       "  '<extra_id_1>',\n",
       "  '<extra_id_2>',\n",
       "  '<extra_id_3>',\n",
       "  '<extra_id_4>',\n",
       "  '<extra_id_5>',\n",
       "  '<extra_id_6>',\n",
       "  '<extra_id_7>',\n",
       "  '<extra_id_8>',\n",
       "  '<extra_id_9>',\n",
       "  '<extra_id_10>',\n",
       "  '<extra_id_11>',\n",
       "  '<extra_id_12>',\n",
       "  '<extra_id_13>',\n",
       "  '<extra_id_14>',\n",
       "  '<extra_id_15>',\n",
       "  '<extra_id_16>',\n",
       "  '<extra_id_17>',\n",
       "  '<extra_id_18>',\n",
       "  '<extra_id_19>',\n",
       "  '<extra_id_20>',\n",
       "  '<extra_id_21>',\n",
       "  '<extra_id_22>',\n",
       "  '<extra_id_23>',\n",
       "  '<extra_id_24>',\n",
       "  '<extra_id_25>',\n",
       "  '<extra_id_26>',\n",
       "  '<extra_id_27>',\n",
       "  '<extra_id_28>',\n",
       "  '<extra_id_29>',\n",
       "  '<extra_id_30>',\n",
       "  '<extra_id_31>',\n",
       "  '<extra_id_32>',\n",
       "  '<extra_id_33>',\n",
       "  '<extra_id_34>',\n",
       "  '<extra_id_35>',\n",
       "  '<extra_id_36>',\n",
       "  '<extra_id_37>',\n",
       "  '<extra_id_38>',\n",
       "  '<extra_id_39>',\n",
       "  '<extra_id_40>',\n",
       "  '<extra_id_41>',\n",
       "  '<extra_id_42>',\n",
       "  '<extra_id_43>',\n",
       "  '<extra_id_44>',\n",
       "  '<extra_id_45>',\n",
       "  '<extra_id_46>',\n",
       "  '<extra_id_47>',\n",
       "  '<extra_id_48>',\n",
       "  '<extra_id_49>',\n",
       "  '<extra_id_50>',\n",
       "  '<extra_id_51>',\n",
       "  '<extra_id_52>',\n",
       "  '<extra_id_53>',\n",
       "  '<extra_id_54>',\n",
       "  '<extra_id_55>',\n",
       "  '<extra_id_56>',\n",
       "  '<extra_id_57>',\n",
       "  '<extra_id_58>',\n",
       "  '<extra_id_59>',\n",
       "  '<extra_id_60>',\n",
       "  '<extra_id_61>',\n",
       "  '<extra_id_62>',\n",
       "  '<extra_id_63>',\n",
       "  '<extra_id_64>',\n",
       "  '<extra_id_65>',\n",
       "  '<extra_id_66>',\n",
       "  '<extra_id_67>',\n",
       "  '<extra_id_68>',\n",
       "  '<extra_id_69>',\n",
       "  '<extra_id_70>',\n",
       "  '<extra_id_71>',\n",
       "  '<extra_id_72>',\n",
       "  '<extra_id_73>',\n",
       "  '<extra_id_74>',\n",
       "  '<extra_id_75>',\n",
       "  '<extra_id_76>',\n",
       "  '<extra_id_77>',\n",
       "  '<extra_id_78>',\n",
       "  '<extra_id_79>',\n",
       "  '<extra_id_80>',\n",
       "  '<extra_id_81>',\n",
       "  '<extra_id_82>',\n",
       "  '<extra_id_83>',\n",
       "  '<extra_id_84>',\n",
       "  '<extra_id_85>',\n",
       "  '<extra_id_86>',\n",
       "  '<extra_id_87>',\n",
       "  '<extra_id_88>',\n",
       "  '<extra_id_89>',\n",
       "  '<extra_id_90>',\n",
       "  '<extra_id_91>',\n",
       "  '<extra_id_92>',\n",
       "  '<extra_id_93>',\n",
       "  '<extra_id_94>',\n",
       "  '<extra_id_95>',\n",
       "  '<extra_id_96>',\n",
       "  '<extra_id_97>',\n",
       "  '<extra_id_98>',\n",
       "  '<extra_id_99>']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What effect did the 1936 Soviet Constitution have on the size of the Russia?</s> Many regions in Russia were affected by the Soviet famine of 1932–1933: Volga; Central Black Soil Region; North Caucasus; the Urals; the Crimea; part of Western Siberia; and the Kazak ASSR. With the adoption of the 1936 Soviet Constitution on December 5, 1936, the size of the RSFSR was significantly reduced. The Kazakh ASSR and Kirghiz ASSR were transformed into the Kazakh and Kirghiz Soviet Socialist Republics. The Karakalpak Autonomous Socialist Soviet Republic was transferred to the Uzbek SSR.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding['input_ids'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_encoding = tokenizer(\n",
    "    sample['answers']['text'][0],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=32,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reduced</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(answer_encoding['input_ids'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3915,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = answer_encoding['input_ids']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3915,    1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataset to pytorch tensors using the tokenizer\n",
    "def convert_to_features(example_batch):\n",
    "    # Tokenize contexts and questions (as pairs of inputs)\n",
    "    encodings = tokenizer(\n",
    "        example_batch['question'],\n",
    "        example_batch['context'],\n",
    "        truncation='only_second',\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    answer_texts=[]\n",
    "    for answers in example_batch['answers']:\n",
    "            answer_texts.append(answers['text'])\n",
    "            \n",
    "    print(answer_texts)\n",
    "    # Tokenize answers\n",
    "    answers = tokenizer(\n",
    "        answer_texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=32,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # replace -100 in the labels as we can't decode them.\n",
    "    labels = answers['input_ids']\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    encodings['labels'] = labels\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in the late 1990s', 'singing and dancing', '2003', 'Houston, Texas', 'late 1990s']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get encodings\n",
    "sample_encodings = convert_to_features(dataset['train'][:5])\n",
    "sample_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subtldev/T5-training/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40218377113342285\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "loss = model(input_ids=sample_encodings['input_ids'], attention_mask=sample_encodings['attention_mask'], labels=sample_encodings['labels']).loss\n",
    "print(loss.item())  # 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='16290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  306/16290 02:46 < 2:25:39, 1.83 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=None,\n",
    "    train_dataset=dataset['train'].map(convert_to_features, batched=True),\n",
    "    eval_dataset=dataset['validation'].map(convert_to_features, batched=True)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# create a question and 2 contexts where 1 of the context contains the answer to the question while the other doesn't\n",
    "\n",
    "question = \"who painted the mona lisa?\"\n",
    "context1 = \"The Mona Lisa is a 16th century oil painting created by Leonardo da Vinci. It's held at the Louvre in Paris.\"\n",
    "context2 = \"The Mona Lisa is a 16th century oil painting. It's held at the Louvre in Paris.\"\n",
    "\n",
    "# get the answer\n",
    "res1 = question_answerer(question=question, context=context1)\n",
    "res2 = question_answerer(question=question, context=context2)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer 1: {res1['answer']}\")\n",
    "print(f\"Answer 2: {res2['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
